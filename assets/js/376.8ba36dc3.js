(window.webpackJsonp=window.webpackJsonp||[]).push([[376],{871:function(t,s,a){"use strict";a.r(s);var n=a(30),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"scrapy发送翻页请求"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scrapy发送翻页请求"}},[t._v("#")]),t._v(" scrapy发送翻页请求")]),t._v(" "),a("h5",{attrs:{id:"学习目标"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#学习目标"}},[t._v("#")]),t._v(" 学习目标：")]),t._v(" "),a("ol",[a("li",[t._v("应用 完善并使用Item数据类")]),t._v(" "),a("li",[t._v("应用 构造Request对象，并发送请求")]),t._v(" "),a("li",[t._v("应用 利用meta参数在不同的解析函数中传递数据")])]),t._v(" "),a("hr"),t._v(" "),a("h3",{attrs:{id:"_1-翻页请求的思路"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-翻页请求的思路"}},[t._v("#")]),t._v(" 1. 翻页请求的思路")]),t._v(" "),a("p",[t._v("对于要提取如下图中所有页面上的数据该怎么办？")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/images/scrapy%E7%BF%BB%E9%A1%B5.png",alt:""}})]),t._v(" "),a("p",[t._v("回顾requests模块是如何实现翻页请求的：")]),t._v(" "),a("ol",[a("li",[t._v("找到下一页的URL地址")]),t._v(" "),a("li",[t._v("调用requests.get(url)")])]),t._v(" "),a("p",[t._v("scrapy实现翻页的思路：")]),t._v(" "),a("ol",[a("li",[t._v("找到下一页的url地址")]),t._v(" "),a("li",[t._v("构造url地址的请求，传递给引擎")])]),t._v(" "),a("h3",{attrs:{id:"_2-scrapy实现翻页请求"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-scrapy实现翻页请求"}},[t._v("#")]),t._v(" 2 scrapy实现翻页请求")]),t._v(" "),a("h4",{attrs:{id:"_2-1-实现方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-实现方法"}},[t._v("#")]),t._v(" 2.1 实现方法")]),t._v(" "),a("ol",[a("li",[t._v("确定url地址")]),t._v(" "),a("li",[t._v("构造请求，scrapy.Request(url,callback)\n"),a("ul",[a("li",[t._v("callback：指定解析函数名称，表示该请求返回的响应使用哪一个函数进行解析")])])]),t._v(" "),a("li",[t._v("把请求交给引擎：yield scrapy.Request(url,callback)")])]),t._v(" "),a("h4",{attrs:{id:"_2-2-腾讯招聘爬虫"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-腾讯招聘爬虫"}},[t._v("#")]),t._v(" 2.2 腾讯招聘爬虫")]),t._v(" "),a("blockquote",[a("p",[t._v("通过爬取腾讯招聘的页面的招聘信息,学习如何实现翻页请求")]),t._v(" "),a("p",[t._v("地址："),a("a",{attrs:{href:"http://hr.tencent.com/position.php",target:"_blank",rel:"noopener noreferrer"}},[t._v("http://hr.tencent.com/position.php"),a("OutboundLink")],1)])]),t._v(" "),a("h5",{attrs:{id:"思路分析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#思路分析"}},[t._v("#")]),t._v(" 思路分析：")]),t._v(" "),a("ol",[a("li",[t._v("获取首页的数据")]),t._v(" "),a("li",[t._v("寻找下一页的地址，进行翻页，获取数据")])]),t._v(" "),a("h5",{attrs:{id:"注意"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#注意"}},[t._v("#")]),t._v(" 注意：")]),t._v(" "),a("ol",[a("li",[t._v("可以在settings中设置ROBOTS协议")])]),t._v(" "),a("div",{staticClass:"language-Python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# False表示忽略网站的robots.txt协议，默认为True")]),t._v("\n    ROBOTSTXT_OBEY "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v(" \n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("ol",{attrs:{start:"2"}},[a("li",[t._v("可以在settings中设置User-Agent：")])]),t._v(" "),a("div",{staticClass:"language-Python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# scrapy发送的每一个请求的默认UA都是设置的这个User-Agent")]),t._v("\n    USER_AGENT "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'")]),t._v(" \n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("h4",{attrs:{id:"_2-3-代码实现"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-代码实现"}},[t._v("#")]),t._v(" 2.3 代码实现")]),t._v(" "),a("p",[t._v("在爬虫文件的parse方法中：")]),t._v(" "),a("div",{staticClass:"language-Python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 提取下一页的href并拼接url")]),t._v("\n    next_url "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'https://hr.tencent.com/'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'//a[text()=\"下一页\"]/@href'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 判断是否是最后一页")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'//a[text()=\"下一页\"]/@href'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'javascript:;'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造scrapy.Request对象，并yield给引擎")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 利用callback参数指定该Request对象之后获取的响应用哪个函数进行解析")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 利用meta参数将本函数中提取的数据传递给callback指定的函数")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 注意这里是yield")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Request"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("next_url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" callback"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" \n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br")])]),a("h4",{attrs:{id:"_2-4-scrapy-request的更多参数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-scrapy-request的更多参数"}},[t._v("#")]),t._v(" 2.4 scrapy.Request的更多参数")]),t._v(" "),a("div",{staticClass:"language-Python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Request"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("callback"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("method"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"GET"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("headers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("body"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("cookies"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\\\nmeta"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("dont_filter"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("h5",{attrs:{id:"参数解释"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参数解释"}},[t._v("#")]),t._v(" 参数解释")]),t._v(" "),a("ol",[a("li",[t._v("中括号中的参数为可选参数")]),t._v(" "),a("li",[t._v("callback：表示当前的url的响应交给哪个函数去处理")]),t._v(" "),a("li",[t._v("meta：实现数据在不同的解析函数中传递，meta默认带有部分数据，比如下载延迟，请求深度等")]),t._v(" "),a("li",[t._v("dont_filter:默认为False，会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动")]),t._v(" "),a("li",[t._v("method：指定POST或GET请求")]),t._v(" "),a("li",[t._v("headers：接收一个字典，其中不包括cookies")]),t._v(" "),a("li",[t._v("cookies：接收一个字典，专门放置cookies")]),t._v(" "),a("li",[t._v("body：接收一个字典，为POST的数据")])]),t._v(" "),a("h3",{attrs:{id:"_3-meta参数的使用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-meta参数的使用"}},[t._v("#")]),t._v(" 3 meta参数的使用")]),t._v(" "),a("h5",{attrs:{id:"meta的形式-字典"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#meta的形式-字典"}},[t._v("#")]),t._v(" meta的形式:字典")]),t._v(" "),a("h5",{attrs:{id:"meta的作用-meta可以实现数据在不同的解析函数中的传递"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#meta的作用-meta可以实现数据在不同的解析函数中的传递"}},[t._v("#")]),t._v(" meta的作用：meta可以实现数据在不同的解析函数中的传递")]),t._v(" "),a("p",[t._v("在爬虫文件的parse方法中，提取详情页增加之前callback指定的parse_detail函数：")]),t._v(" "),a("div",{staticClass:"language-Python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Request"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("detail_url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" callback"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse_detail"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("meta"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"item"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse_detail")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#获取之前传入的item")]),t._v("\n    item "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" resposne"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("meta"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"item"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" \n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br")])]),a("h5",{attrs:{id:"特别注意"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#特别注意"}},[t._v("#")]),t._v(" 特别注意")]),t._v(" "),a("ol",[a("li",[t._v("meta参数是一个字典")]),t._v(" "),a("li",[t._v("meta字典中有一个固定的键"),a("code",[t._v("proxy")]),t._v("，表示代理ip，关于代理ip的使用我们将在scrapy的下载中间件的学习中进行介绍")])]),t._v(" "),a("h3",{attrs:{id:"_4-item的使用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-item的使用"}},[t._v("#")]),t._v(" 4. item的使用")]),t._v(" "),a("h4",{attrs:{id:"_4-1-item能够做什么"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-item能够做什么"}},[t._v("#")]),t._v(" 4.1 Item能够做什么")]),t._v(" "),a("ol",[a("li",[a("p",[t._v("定义item即提前规划好哪些字段需要抓取，scrapy.Field()仅仅是提前占坑，通过item.py能够让别人清楚自己的爬虫是在抓取什么，同时定义好哪些字段是需要抓取的，没有定义的字段不能使用，防止手误")])]),t._v(" "),a("li",[a("p",[t._v("在python大多数框架中，大多数框架都会自定义自己的数据类型(在python自带的数据结构基础上进行封装)，目的是增加功能，增加自定义异常")])])]),t._v(" "),a("h4",{attrs:{id:"_4-2-定义item"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-定义item"}},[t._v("#")]),t._v(" 4.2 定义Item")]),t._v(" "),a("p",[t._v("在items.py文件中定义要提取的字段：")]),t._v(" "),a("div",{staticClass:"language-Python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TencentItem")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" \n    name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 招聘标题")]),t._v("\n    address "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 工作地址")]),t._v("\n    time "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 发布时间")]),t._v("\n    job_content "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 工作职责 ")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br")])]),a("h4",{attrs:{id:"_4-3-使用item"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-使用item"}},[t._v("#")]),t._v(" 4.3 使用Item")]),t._v(" "),a("p",[t._v("Item使用之前需要先导入并且实例化，之后的使用方法和使用字典相同")]),t._v(" "),a("p",[t._v("修改爬虫文件itcast.py：")]),t._v(" "),a("div",{staticClass:"language-Python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" Tencent"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" TencentItem "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 导入Item，注意路径")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse_detail")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        meta_dict "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("meta "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取传入的meta")]),t._v("\n\n        item "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ItcastItem"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 实例化后可直接使用")]),t._v("\n        item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'name'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" meta_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'name'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'address'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" meta_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("'address"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'time'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" meta_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'time'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 加入岗位职责数据")]),t._v("\n        item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job_content'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'//ul[@class=\"squareli\"]/li/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br")])]),a("h5",{attrs:{id:"注意-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#注意-2"}},[t._v("#")]),t._v(" 注意：")]),t._v(" "),a("ol",[a("li",[t._v("from myspider.items import ItcastItem这一行代码中 注意item的正确导入路径，忽略pycharm标记的错误")]),t._v(" "),a("li",[t._v("python中的导入路径要诀：从哪里开始运行，就从哪里开始导入")])]),t._v(" "),a("hr"),t._v(" "),a("h3",{attrs:{id:"总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),a("ol",[a("li",[t._v("完善并使用Item数据类：\n"),a("ol",[a("li",[t._v("在items.py中完善要爬取的字段")]),t._v(" "),a("li",[t._v("在爬虫文件中先导入Item")]),t._v(" "),a("li",[t._v("实力化Item对象后，像字典一样直接使用")])])]),t._v(" "),a("li",[t._v("构造Request对象，并发送请求：\n"),a("ol",[a("li",[t._v("导入scrapy.Request类")]),t._v(" "),a("li",[t._v("在解析函数中提取url")]),t._v(" "),a("li",[t._v("yield scrapy.Request(url, callback=self.parse_detail, meta={})")])])]),t._v(" "),a("li",[t._v("利用meta参数在不同的解析函数中传递数据:\n"),a("ol",[a("li",[t._v("通过前一个解析函数 yield scrapy.Request(url, callback=self.xxx, meta={}) 来传递meta")]),t._v(" "),a("li",[t._v("在self.xxx函数中 response.meta.get('key', '') 或 response.meta['key'] 的方式取出传递的数据")])])])])])}),[],!1,null,null,null);s.default=e.exports}}]);